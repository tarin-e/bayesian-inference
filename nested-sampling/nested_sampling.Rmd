---
title: "Untitled"
output: html_document
date: "2023-10-17"
---

The USA Junior Chess Olympics Research: Developing Memory and Verbal Reasoning” (New
Horizons for Learning, April 2001; (available at www.newhorizons.org) describes a study in
which sixth-grade students who had not previously played chess participated in a program in
which they took chess lessons and played chess daily for 9 months. Each student took a memory
test (the Test of Cognitive Skills) before starting the chess program and again at the end of
the 9-month period. 

The data below has the differences in scores after the chess program and
before, for 12 students.

```{r}
data = list(x=c(340, 180, 210, 100, 100, 225, 90,
225, 240, 55, -35, 5), N=12)
```

2a) Do a classical t-test using R (this should be very easy). Write down the p-value, and write
down the notation for what this probability is (i.e., it’s the probability of what conditional
on what)

```{r}
t.test(data$x)
```

Alternatively

```{r}
t_test = mean(data$x) / (sd(data$x) / sqrt(data$N))
t_test
```


The p-value is the probability of a more extreme value of the test statistic if H0 is true.

t = x_mean / s / sqrt(N)

where s is the sample standard deviation (the version with N − 1 in the denominator).

The observed value of t was 4.564. The p-value is the probability of a more extreme value than this, under H0:
  P (t ≥ 4.564 or t ≤ −4.564 | H0)

2b) Do Bayesian model comparison of H0 vs. H1. You can compute marginal likelihoods using
either Nested Sampling or another method if you prefer. For H1, use a Uniform(-1000000,
1000000) prior for μ. Calculate the posterior odds ratio assuming a prior odds ratio of 1,
and write down the probability notation for what this quantity is.

H0 : mu = 0
H1 : mu /= 0

Todo: setup the file such that we run all the files from here

```{r}
source("ns_tutorial/nested-sampling-tutorial.R")

results = read.csv("ns-output.csv")
```

Results from NS run on H1

Marginal likelihood: ln(Z) = -87.32934 +- 0.3664631.
Information: H = 13.42952 nats.
Effective posterior sample size = 498.
Posterior samples saved in ns-posterior-samples.csv.

Results from NS run on H0

Marginal likelihood: ln(Z) = -83.29037 +- 0.1878688.
Information: H = 3.529469 nats.
Effective posterior sample size = 391.
Posterior samples saved in ns-posterior-samples.csv.

Posterior Odds

P(H0|x) / P(H1|x) = (P(H0) / P(H1)) * (P(x|H0) / P(x|H1))

= 1 * (exp(-83.29037) / exp(-87.32934))

= 56.651

We are 57x more in favour of the null hypothesis that mu = 0. This is due to the alternative hypothesis having a mega wide prior.

2c) The apparent contradiction between the results of (i) and (ii) is sometimes called the Jeffreys-Lindley paradox. Explain, in words and/or diagrams, why the conclusion of part (ii) is actually valid for a reasoner whose prior beliefs are really described by the Uniform(-1000000, 1000000) prior

Because if mu ~ Uniform(-1000000, 1000000), we are equally likely to get extreme values for mu which are much further from 0 or the true mu value. This makes it much more likely that the null model where mu = 0 is favourable than mu /= 0. In other words model selection is very sensitive to the prior. 

The conclusion in ii) is valid if we have not seen our data.

The t-test on the otherhand is sensitive to the data.

2d) Harold Jeffreys had a suggestion for problems like this. The parameter σ already sets a scale for the problem (“we’re dealing with stuff around 100–300”), so conditional on σ, we can use σ to give us a suitable non-extreme prior for μ. 

He suggested using a Cauchy distribution because of the heavy tails. Re-do part (ii) using μ ∼ Cauchy (0, σ) as the prior for μ given σ. Hint: Use κ = μ/σ as a parameter instead of μ, and then let μ ← κσ

```{r}
source("ns_tutorial/nested-sampling-tutorial.R")

results = read.csv("ns-output.csv")
```

Marginal likelihood: ln(Z) = -78.73237 +- 0.2186847.
Information: H = 4.782301 nats.
Effective posterior sample size = 505.
Posterior samples saved in ns-posterior-samples.csv.
