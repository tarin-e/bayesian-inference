### Assignment 4
#### Tarin Eccleston

In this question, perform model averaging/selection to try to infer whether I used runif() or rexp().
Let U be the proposition that it was runif() and E be the proposition that it was rexp(). If U is
true, then the prior and sampling distribution are

log b ∼ Normal (0, 1)
xi | b ∼ Uniform(0, b)

If E is true, then the prior and sampling distribution are

log λ ∼ Normal (0, 1) (3)
xi | λ ∼ Exponential(λ)


1a) Both models E and U imply prior predictive distributions for the data, and hence
the data mean x_bar. Would the two prior predictive distributions for̄ x_bar be the same or different?
Explain your answer.

Both E and U will have different prior predictive distributions (ppd) since both sampling distributions for E and U are different with exponential and uniform distributions respectively. This can be shown by the ppd formula shown before.

Todo: double check this

Todo: insert formula here

1b) Part (a) implies that learning only x_bar would provide some information about whether
E or U is true. Does this seem reasonable to you?

Yes it does. Todo: expand on this

1c) Write down analytical expressions for the marginal likelihoods p(x | U ) and p(x | E).
Retain all constant factors.???

This is wrong. Well slightly. Make sure to refer to screenshot.

$$
p(x | U) =  \frac{1}{\sqrt{2\pi}} \int_{0}^{\infty} \frac{1}{b^2} e^{-\frac{1}{2} (\ln{b})^2} db
$$
$$
p(x | E) =  \frac{1}{\sqrt{2\pi}} \int_{0}^{\infty} e^{-(\frac{1}{2} (\ln{b})^2 + \lambda)} d\lambda
$$
1d) Numerically find the values of the two marginal likelihoods.

```{r}
source("../nested-sampling/ns_assignment/nested-sampling-assignment.R")
```

for p(x | U):

Marginal likelihood: ln(Z) = -0.3483763 +- 0.05943507.
Information: H = 0.3532527 nats.
Effective posterior sample size = 337.
Posterior samples saved in ns-posterior-samples.csv.

for p(x | E):

Marginal likelihood: ln(Z) = -7.059105 +- 0.07194591.
Information: H = 0.5176214 nats.
Effective posterior sample size = 356.
Posterior samples saved in ns-posterior-samples.csv.

1e) ???

Find the Bayes Factor (either way around) and also the posterior probabilities of U
and E, assuming prior probabilities of 1/2 each.

```{r}
# Bayes factor : U/E
marg_U = exp(-0.3483763)
marg_E = exp(-7.059105)

bayes_factor =  marg_U / marg_E
bayes_factor
```


The uniform distribution is ~800 times more likely to produce observed data compared to exponential distribution.

Prior probabilities = 1/2

```{r}
prior_probs = c(1/2, 1/2)
lik = c(-0.3483763, -7.059105)

h = prior_probs * lik
Z = sum(h)

posterior_odds = h/Z
posterior_odds
```

1f) If p(b | U ) were made much wider, the Bayes Factor would strongly favour E. Explain why this occurs.

We have limited data therefore our posterior distributions are sensitive to the choice of prior. If our uniform prior margins for p(b | U ) widen, we effectively decrease the prior probabilities for the same region which overlap the likelihood. We are also widening the likelihood since b is the standard deviation.  

As a result our posterior probabilities for  p(b | U, x) decreases. Hence our Bayes factor then decreases and favours E instead of U.

Todo: link this up a bit better.

2a)  Implement Model A for Nested Sampling, and run it. In your answer, provide the fol-
lowing: (i) num_params, (ii), parameter_names, (iii) us_to_params(), (iv) log_likelihood(), and (v) the estimated value of the marginal likelihood P (y | A) and its uncertainty.

```{r}
source("../nested-sampling/ns_assignment/nested-sampling-assignment.R")
```

### Output code for Model A 

Marginal likelihood: ln(Z) = -685.036 +- 0.3414809.
Information: H = 11.66092 nats.
Effective posterior sample size = 594.
Posterior samples saved in ns-posterior-samples.csv.

### Answers from ns-model

num_params = 3

parameter_names = c("beta_0", "beta_1", "sigma")

colorado_df = read.csv("../data/colorado.csv")

us_to_params = function(us)
{
    params = rep(NA, num_params)

    names(params) = parameter_names

    params["beta_0"] = exp(qnorm(us[1], 0, 5))
    params["beta_1"] = qnorm(us[2], 0, params["beta_0"])
    params["sigma"] = exp(qnorm(us[3], log(params["beta_0"]),1))

    return(params)
}

log_likelihood = function(params)
{
    mu = params["beta_0"] + params["beta_1"]*colorado_df$t
    logL = sum(dnorm(x = colorado_df$y, mean = mu, sd = params["sigma"], log = TRUE))
    return(logL)
}

#### Marginal Likelihood

```{r}
Z_MA_mu = exp(-685.036)
Z_MA_mu
```

#### Marginal Likelihood Error Range

```{r}
Z_MA_error = c(exp(-685.036 + 0.3414809), exp(-685.036 - 0.3414809))
Z_MA_error
```


```{r}
source("../nested-sampling/ns_assignment/nested-sampling-assignment.R")
```

### Output code for Model B 

Marginal likelihood: ln(Z) = -683.9028 +- 0.366068.
Information: H = 13.40058 nats.
Effective posterior sample size = 712.
Posterior samples saved in ns-posterior-samples.csv.

### Answers from ns-model

num_params = 4

parameter_names = c("beta_0", "beta_1", "beta_2", "sigma")

colorado_df = read.csv("../data/colorado.csv")

us_to_params = function(us)
{
    params = rep(NA, num_params)

    names(params) = parameter_names

    params["beta_0"] = exp(qnorm(us[1], 0, 5))
    params["beta_1"] = qnorm(us[2], 0, params["beta_0"])
    params["beta_2"] = qnorm(us[3], 0, params["beta_0"])
    params["sigma"] = exp(qnorm(us[4], log(params["beta_0"]),1))

    return(params)
}

log_likelihood = function(params)
{
    mu = params["beta_0"] + params["beta_1"]*colorado_df$t + (colorado_df$t >= 0.875)*params["beta_2"]*(colorado_df$t - 0.875)
    logL = sum(dnorm(x = colorado_df$y, mean = mu, sd = params["sigma"], log = TRUE))
    return(logL)
}

#### Marginal Likelihood

```{r}
Z_MB_mu = exp(-683.9028)
Z_MB_mu
```

#### Marginal Likelihood Error Range

```{r}
Z_MB_error = c(exp(-683.9028 + 0.366068), exp(-683.9028 - 0.366068))
Z_MB_error
```

```{r}
source("../nested-sampling/ns_assignment/nested-sampling-assignment.R")
```

### Output code for Model C 

Marginal likelihood: ln(Z) = -680.6652 +- 0.3902254.
Information: H = 15.22758 nats.
Effective posterior sample size = 700.
Posterior samples saved in ns-posterior-samples.csv.

### Answers from ns-model

num_params = 4

parameter_names = c("beta_0", "beta_1", "beta_2", "sigma")

colorado_df = read.csv("../data/colorado.csv")

us_to_params = function(us)
{
    params = rep(NA, num_params)

    names(params) = parameter_names

    params["beta_0"] = exp(qnorm(us[1], 0, 5))
    params["beta_1"] = qnorm(us[2], 0, params["beta_0"])
    params["beta_2"] = qnorm(us[3], 0, params["beta_0"])
    params["sigma"] = exp(qnorm(us[4], log(params["beta_0"]),1))

    return(params)
}

log_likelihood = function(params)
{
    mu = params["beta_0"] + params["beta_1"]*colorado_df$t + params["beta_2"]*(colorado_df$t)^2
    logL = sum(dnorm(x = colorado_df$y, mean = mu, sd = params["sigma"], log = TRUE))
    return(logL)
}

#### Marginal Likelihood

```{r}
Z_MC_mu = exp(-680.6652)
Z_MC_mu
```

#### Marginal Likelihood Error Range

```{r}
Z_MC_error = c(exp(-680.6652 + 0.3902254), exp(-680.6652 - 0.3902254))
Z_MC_error
```

2d) If the three models have prior probabilities P (A) = P (B) = P (C) = 1/3, find the
super-duper-marginalised likelihood P (y | A ∨ B ∨ C) and the three posterior probabilities.

this is the same as p(y)

```{r}
prior = c(1/3, 1/3, 1/3)
marg_lik = c(exp(-685.036), exp(-683.9028), exp(-680.6652))

h_super = prior * marg_lik
Z_super = sum(h_super)
Z_super
```

```{r}
posterior_probabilities = h_super / Z_super
posterior_probabilities
```





