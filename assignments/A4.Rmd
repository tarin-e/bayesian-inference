---
output:
  html_document: default
  pdf_document: default
---
### Assignment 4
#### Tarin Eccleston

1a) Both models E and U imply prior predictive distributions for the data, and hence
the data mean x_bar. Would the two prior predictive distributions for̄ x_bar be the same or different?
Explain your answer.

Both E and U will have different prior predictive distributions (ppd) for x_bar since both sampling distributions for E and U are different with exponential and uniform distributions respectively. The prior distributions remain the same. The general ppd formula shown below.

$$
p(\bar{x}) =  \int_{0}^{\infty} (\prod_{i = 1}^{5} p(x_i| \theta)) p(\theta) d\theta
$$


1b) Part (a) implies that learning only x_bar would provide some information about whether
E or U is true. Does this seem reasonable to you?

Yes it does. If the main difference between E or U is the choice of uniform vs exponential sampling distributions, and we are given x_bar, then we can evaluate the posterior distribution in terms of x_bar. E or U with the highest posterior distribution would hence be the most true model.

1c) Write down analytical expressions for the marginal likelihoods p(x | U ) and p(x | E).
Retain all constant factors.

$$
p(\bar{x} | U) =  \frac{1}{\sqrt{2\pi}} \int_{x_{max}}^{\infty} (\prod_{i = 1}^{5}\frac{1}{b})\frac{1}{b} e^{-\frac{1}{2} (\ln{b})^2} db
$$

Likelihood function is not 1/b for all x. 

$$
p(\bar{x} | E) =  \frac{1}{\sqrt{2\pi}} \int_{0}^{\infty} \lambda^4 e^{-(\frac{1}{2} (\ln{\lambda})^2 + 5\lambda\bar{x})} d\lambda
$$
1d) Numerically find the values of the two marginal likelihoods.

### Output code for Model U

Marginal likelihood: ln(Z) = -0.3483763 +- 0.05943507.
Information: H = 0.3532527 nats.
Effective posterior sample size = 337.
Posterior samples saved in ns-posterior-samples.csv.

### Model U ns-model

```{r eval=FALSE} 
num_params = 1

parameter_names = c("theta")

data = list(x=c(0.610164901707321, 1.99984208494425, 1.50817369576544, 0.707493807654828,
                           1.49413506453857), N=5)

us_to_params = function(us)
{
    params = rep(NA, num_params)

    names(params) = parameter_names

    params["theta"] = exp(qnorm(us[1], 0, 1))

    return(params)
}

log_likelihood = function(params)
{
    logL = sum(dunif(x = data$x, min = 0, max = params["theta"], log = TRUE)[is.finite(dunif(x = data$x, min = 0, max = params["theta"], log = TRUE))])
    return(logL)
}
```

### Output code for Model E

Marginal likelihood: ln(Z) = -7.059105 +- 0.07194591.
Information: H = 0.5176214 nats.
Effective posterior sample size = 356.
Posterior samples saved in ns-posterior-samples.csv.

### Model E ns-model

```{r eval=FALSE}
num_params = 1

parameter_names = c("theta")

data = list(x=c(0.610164901707321, 1.99984208494425, 1.50817369576544, 0.707493807654828,
                           1.49413506453857), N=5)

us_to_params = function(us)
{
    params = rep(NA, num_params)

    names(params) = parameter_names

    params["theta"] = exp(qnorm(us[1], 0, 1))

    return(params)
}

log_likelihood = function(params)
{
    logL = sum(dexp(x = data$x, rate = params["theta"], log = TRUE))
    return(logL)
}
```


```{r}
Z_U = exp(-0.3483763)
Z_U
```

```{r}
Z_E = exp(-7.059105)
Z_E
```

1e)

Find the Bayes Factor (either way around) and also the posterior probabilities of U
and E, assuming prior probabilities of 1/2 each.

```{r}
# Bayes factor : U/E
bayes_factor =  Z_U / Z_E
bayes_factor
```

The uniform distribution is ~800 times more likely to produce observed data compared to exponential distribution.

Prior probabilities for U and E: 1/2

```{r}
prior_probs = c(1/2, 1/2)
lik = c(exp(-0.3483763), exp(-7.059105))

h = prior_probs * lik
Z = sum(h)

posterior_odds = h/Z
posterior_odds
```

1f) If p(b | U ) were made much wider, the Bayes Factor would strongly favour E. Explain why this occurs.

We have limited data therefore our posterior distributions are sensitive to the choice of prior. If our uniform prior margins for p(b | U ) widen, we effectively decrease the prior probabilities for the same region which overlap the likelihood. We are also widen the likelihood since b is the standard deviation.  

As a result our posterior probabilities for  p(b | U, x) decreases. Hence our Bayes factor then decreases and favours E instead of U.

2a)  Implement Model A for Nested Sampling, and run it. In your answer, provide the following: (i) num_params, (ii), parameter_names, (iii) us_to_params(), (iv) log_likelihood(), and (v) the estimated value of the marginal likelihood P (y | A) and its uncertainty.

### Output code for Model A 

Marginal likelihood: ln(Z) = -685.036 +- 0.3414809.
Information: H = 11.66092 nats.
Effective posterior sample size = 594.
Posterior samples saved in ns-posterior-samples.csv.

### Model A ns-model

```{r eval=FALSE}
num_params = 3

parameter_names = c("beta_0", "beta_1", "sigma")

colorado_df = read.csv("../data/colorado.csv")

us_to_params = function(us)
{
    params = rep(NA, num_params)

    names(params) = parameter_names

    params["beta_0"] = exp(qnorm(us[1], 0, 5))
    params["beta_1"] = qnorm(us[2], 0, params["beta_0"])
    params["sigma"] = exp(qnorm(us[3], log(params["beta_0"]),1))

    return(params)
}

log_likelihood = function(params)
{
    mu = params["beta_0"] + params["beta_1"]*colorado_df$t
    logL = sum(dnorm(x = colorado_df$y, mean = mu, sd = params["sigma"], log = TRUE))
    return(logL)
}
```

#### Marginal Likelihood

```{r}
Z_MA_mu = exp(-685.036)
Z_MA_mu
```

#### Marginal Likelihood Error Range

```{r}
Z_MA_error = c(exp(-685.036 - 0.3414809), exp(-685.036 + 0.3414809))
Z_MA_error
```

### Output code for Model B 

Marginal likelihood: ln(Z) = -683.9028 +- 0.366068.
Information: H = 13.40058 nats.
Effective posterior sample size = 712.
Posterior samples saved in ns-posterior-samples.csv.

### Model B ns-model

```{r eval=FALSE}
num_params = 4

parameter_names = c("beta_0", "beta_1", "beta_2", "sigma")

colorado_df = read.csv("../data/colorado.csv")

us_to_params = function(us)
{
    params = rep(NA, num_params)

    names(params) = parameter_names

    params["beta_0"] = exp(qnorm(us[1], 0, 5))
    params["beta_1"] = qnorm(us[2], 0, params["beta_0"])
    params["beta_2"] = qnorm(us[3], 0, params["beta_0"])
    params["sigma"] = exp(qnorm(us[4], log(params["beta_0"]),1))

    return(params)
}

log_likelihood = function(params)
{
    mu = params["beta_0"] + params["beta_1"]*colorado_df$t + (colorado_df$t >= 0.875)*params["beta_2"]*(colorado_df$t - 0.875)
    logL = sum(dnorm(x = colorado_df$y, mean = mu, sd = params["sigma"], log = TRUE))
    return(logL)
}
```

#### Marginal Likelihood

```{r}
Z_MB_mu = exp(-683.9028)
Z_MB_mu
```

#### Marginal Likelihood Error Range

```{r}
Z_MB_error = c(exp(-683.9028 - 0.366068), exp(-683.9028 + 0.366068))
Z_MB_error
```

### Output code for Model C 

Marginal likelihood: ln(Z) = -680.6652 +- 0.3902254.
Information: H = 15.22758 nats.
Effective posterior sample size = 700.
Posterior samples saved in ns-posterior-samples.csv.

### Model C ns-model

```{r eval=FALSE}
num_params = 4

parameter_names = c("beta_0", "beta_1", "beta_2", "sigma")

colorado_df = read.csv("../data/colorado.csv")

us_to_params = function(us)
{
    params = rep(NA, num_params)

    names(params) = parameter_names

    params["beta_0"] = exp(qnorm(us[1], 0, 5))
    params["beta_1"] = qnorm(us[2], 0, params["beta_0"])
    params["beta_2"] = qnorm(us[3], 0, params["beta_0"])
    params["sigma"] = exp(qnorm(us[4], log(params["beta_0"]),1))

    return(params)
}

log_likelihood = function(params)
{
    mu = params["beta_0"] + params["beta_1"]*colorado_df$t + params["beta_2"]*(colorado_df$t)^2
    logL = sum(dnorm(x = colorado_df$y, mean = mu, sd = params["sigma"], log = TRUE))
    return(logL)
}
```

#### Marginal Likelihood

```{r}
Z_MC_mu = exp(-680.6652)
Z_MC_mu
```

#### Marginal Likelihood Error Range

```{r}
Z_MC_error = c(exp(-680.6652 - 0.3902254), exp(-680.6652 + 0.3902254))
Z_MC_error
```

2d) If the three models have prior probabilities P (A) = P (B) = P (C) = 1/3, find the
super-duper-marginalised likelihood P (y | A ∨ B ∨ C) and the three posterior probabilities.

this is the same as p(y)

```{r}
prior = c(1/3, 1/3, 1/3)
marg_lik = c(exp(-685.036), exp(-683.9028), exp(-680.6652))

h_super = prior * marg_lik
Z_super = sum(h_super)
Z_super
```

```{r}
posterior_probabilities = h_super / Z_super
posterior_probabilities
```


